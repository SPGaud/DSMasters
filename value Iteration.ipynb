{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration gridworld\n",
    "\n",
    "\n",
    "In project we will implement the Value Iteration algorithm to compute an optimal policy for three different (but related) Markov Decision Processes. The pseudo-code for the algorithm is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998). \n",
    "\n",
    "<img src=\"images/value_iteration.png\" style=\"width: 800px;\"/>\n",
    "\n",
    "The set $\\mathcal{S}$ contains all non-terminal states, whereas $\\mathcal{S}^+$ is the set of all states (terminal and non-terminal). The reward $r = r(s, a, s')$ is the expected immediate reward on transition from state $s$ to the next state $s'$ under action $a$. \n",
    "\n",
    "<img src=\"images/bombs and gold numbers.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/>\n",
    "\n",
    "The three problems you will solve use variants of the gridworld environment shown on the left. You should be familiar with this environment from the lectures and from your previous lab exercise. The grid squares in the figure are numbered as shown. In all three problems, the following is true: \n",
    "\n",
    "**Actions available:** The agent has four possible actions in each grid square. These are _west_, _north_, _south_, and _east_. If the direction of movement is blocked by a wall (for example, if the agent executes action south at grid square 1), the agent remains in the same grid square. \n",
    "\n",
    "**Collecting gold:** On its first arrival at a grid square that contains gold, the agent collects the gold. In order to collect the gold, the agent needs to transition into the grid square (containing the gold) from a different grid square. \n",
    "\n",
    "**Hitting the bomb:** On arrival at a grid square that contains the bomb, the agent activates the bomb. \n",
    "\n",
    "** Terminal states:** The game terminates when all gold is collected or when the bomb is activated. In Exercises 1 and 2, you can define terminal states to be grid squares 18 and 23. In Exercise 3, you will need to define terminal state(s) differently.\n",
    "\n",
    "\n",
    "### Instructions ###\n",
    "Set parameter $\\theta$ to 1 to the power of -10.\n",
    "\n",
    "Set all initial state values $V(s)$ to zero.\n",
    "\n",
    "\n",
    "We will use the reward function: $-1$ for each navigation action (including when the action results in hitting the wall), an additional $+10$ for collecting each piece of gold, and an additional $-10$ for activating the bomb. For example, the immediate reward for transitioning into a square with gold is $-1 + 10 = +9$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic environment\n",
    "\n",
    "In this first exercise, the agent is able to move in the intended direction with certainty. For example, if it executes action _north_ in grid square 0, it will transition to grid square 5 with probability 1. In other words, we have a deterministic environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld:\n",
    "    def __init__(self):\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.gold_reward = 10\n",
    "        self.bomb_reward = -10\n",
    "        self.gold_positions = np.array([23])\n",
    "        self.bomb_positions = np.array([18])\n",
    "        self.random_move_probability = 0.2\n",
    "\n",
    "        self.actions = [\"UP\", \"RIGHT\", \"DOWN\", \"LEFT\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.num_fields = self.num_cols * self.num_rows\n",
    "        \n",
    "        self.rewards = np.zeros(shape=self.num_fields)\n",
    "        self.rewards[self.bomb_positions] = self.bomb_reward\n",
    "        self.rewards[self.gold_positions] = self.gold_reward\n",
    "\n",
    "        self.step = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "        self.availableMoves = np.zeros(self.num_actions)\n",
    "        self.stochasticMoves = np.zeros(shape=(self.num_fields,self.num_actions))\n",
    "        \n",
    "    def availablePositions(self, oldstate):\n",
    "        # Determine available positions and check whether the agent hits a wall.\n",
    "        #[up,right,down,left]\n",
    "        \n",
    "        #up\n",
    "        candidate_position = oldstate + self.num_cols\n",
    "        if candidate_position < self.num_fields:\n",
    "            self.availableMoves[0] = candidate_position\n",
    "        else:\n",
    "            self.availableMoves[0] = oldstate\n",
    "        \n",
    "        #right\n",
    "        candidate_position = oldstate + 1\n",
    "        if candidate_position % self.num_cols > 0:\n",
    "            self.availableMoves[1] = candidate_position\n",
    "        else:\n",
    "            self.availableMoves[1] = oldstate\n",
    "        \n",
    "        #down\n",
    "        candidate_position = oldstate - self.num_cols\n",
    "        if candidate_position >= 0:\n",
    "            self.availableMoves[2] = candidate_position\n",
    "        else:\n",
    "            self.availableMoves[2] = oldstate\n",
    "        \n",
    "        #left\n",
    "        candidate_position = oldstate - 1\n",
    "        if candidate_position % self.num_cols < self.num_cols - 1:\n",
    "            self.availableMoves[3] = candidate_position\n",
    "        else:\n",
    "            self.availableMoves[3] = oldstate\n",
    "        \n",
    "        \n",
    "        return self.availableMoves\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        # The following statement returns a boolean. It is 'True' when the agent_position\n",
    "        # coincides with any bomb_positions or gold_positions.\n",
    "        return np.append(self.bomb_positions, self.gold_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to play the game\n",
    "def play(environment, episodes=500):\n",
    "    deter = vcalc(environment)\n",
    "    for episode in range(0, episodes):\n",
    "        environment.reset()\n",
    "        \n",
    "        for i in range(environment.num_fields):\n",
    "            state = i\n",
    "            positions = environment.availablePositions(state)\n",
    "\n",
    "            vtab, bestpol = deter.buildValTable(state, positions, environment)\n",
    "\n",
    "    return vtab, bestpol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to calculate value iteration\n",
    "class vcalc:\n",
    "    def __init__(self, environment, epsilon=0.05, alpha=0.1, gamma=1):\n",
    "        self.environment = environment\n",
    "\n",
    "        self.valtable = np.zeros(shape=(self.environment.num_fields))\n",
    "        self.transProb = np.zeros(shape=(self.environment.num_fields, self.environment.num_actions,self.environment.num_fields))\n",
    "        self.bestPolicy = np.empty(shape=(self.environment.num_fields),dtype=str)\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    def calcTransProbs ():\n",
    "        pass\n",
    "    \n",
    "    def buildValTableStoch(self,state,positions, environment):\n",
    "        #get rewards of neigbours\n",
    "        \n",
    "        positions = [int(x) for x in positions]\n",
    "        reward = np.empty(len(positions))\n",
    "        for i in range(len(positions)):\n",
    "\n",
    "            reward[i] = environment.rewards[int(positions[i])]-1\n",
    "\n",
    "        \n",
    "        if (state==environment.is_terminal_state()).any():\n",
    "            \n",
    "            self.valtable[state] = 0\n",
    "            \n",
    "            #so passes assessment check\n",
    "            self.bestPolicy[state] = 'n'\n",
    "            \n",
    "        else:\n",
    "            availableVs = self.valtable[positions]\n",
    "\n",
    "            reward_x_v = reward + availableVs\n",
    "            \n",
    "            #calculate stochastic v values for neighbour states\n",
    "            stochV = np.zeros(4)\n",
    "            \n",
    "            stochV[0] = 0.85*reward_x_v[0] + 0.05*reward_x_v[1] + 0.05*reward_x_v[2] +0.05*reward_x_v[3]\n",
    "\n",
    "            stochV[1] = 0.05*reward_x_v[0] + 0.85*reward_x_v[1] + 0.05*reward_x_v[2] +0.05*reward_x_v[3]\n",
    "\n",
    "            stochV[2] = 0.05*reward_x_v[0] + 0.05*reward_x_v[1] + 0.85*reward_x_v[2] +0.05*reward_x_v[3]\n",
    "\n",
    "            stochV[3] = 0.05*reward_x_v[0] + 0.05*reward_x_v[1] + 0.05*reward_x_v[2] +0.85*reward_x_v[3]\n",
    "            \n",
    "\n",
    "            stochV = np.array(stochV)\n",
    "            \n",
    "            max_v_value = np.max(stochV)\n",
    "             \n",
    "            max_v_pos_list = list(np.nonzero(stochV == max_v_value))[0]\n",
    "        \n",
    "            max_v_pos = random.choice(max_v_pos_list)\n",
    "            \n",
    "            policies = ['n','e','s','w']\n",
    "            \n",
    "            self.valtable[state] = max_v_value\n",
    "            self.bestPolicy[state] = policies[max_v_pos]\n",
    "\n",
    "        \n",
    "        return self.valtable,self.bestPolicy\n",
    "        \n",
    "    \n",
    "    def buildValTable(self,state,positions, environment):\n",
    "        #rewards for all positions\n",
    "        #calculate rewards for all posible moves\n",
    "        #builds the v table\n",
    "        \n",
    "        positions = [int(x) for x in positions]\n",
    "        reward = np.empty(len(positions))\n",
    "        \n",
    "        #calc v for nearbouring states\n",
    "        \n",
    "        #get rewards for neighbouring states\n",
    "        for i in range(len(positions)):\n",
    "\n",
    "            reward[i] = environment.rewards[int(positions[i])]-1\n",
    "            \n",
    "        #check if terminal state and v is 0\n",
    "        if (state==environment.is_terminal_state()).any():\n",
    "            \n",
    "            self.valtable[state] = 0\n",
    "            \n",
    "            #so passes assessment check\n",
    "            self.bestPolicy[state] = 'n'\n",
    "            \n",
    "        #calculate v values by adding reward to vs and return max  \n",
    "        else:\n",
    "            availableVs = self.valtable[positions]\n",
    "\n",
    "            reward_x_v = reward + availableVs\n",
    "\n",
    "            max_v_value = np.max(reward_x_v)\n",
    "\n",
    "            max_v_pos = int(np.where(reward_x_v == max_v_value)[0][0])\n",
    "            \n",
    "            policies = ['n','e','s','w']\n",
    "            \n",
    "            self.valtable[state] = max_v_value\n",
    "            self.bestPolicy[state] = policies[max_v_pos]\n",
    "\n",
    "        \n",
    "        return self.valtable,self.bestPolicy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "f5d99d08008f92f1982b39628d92f411",
     "grade": false,
     "grade_id": "cell-2e05bee97ef409be",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7. 8. 9. 0. 9.]\n",
      " [6. 7. 8. 0. 8.]\n",
      " [5. 6. 7. 6. 7.]\n",
      " [4. 5. 6. 5. 6.]\n",
      " [3. 4. 5. 4. 5.]]\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n"
     ]
    }
   ],
   "source": [
    "# Now we will use the code to compute the values of policy and v using the value iteration algorithm\n",
    "\n",
    "environment = Gridworld()\n",
    "\n",
    "v,policy = play(environment, episodes=500)\n",
    "\n",
    "vdet = np.fliplr(v[::-1].reshape(5,5))\n",
    "print(vdet)\n",
    "\n",
    "\n",
    "#policy table\n",
    "poldet = np.fliplr(policy[::-1].reshape(5,5))\n",
    "print(poldet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the data types of your solution! \n",
    "The following tests allow check whether the variables `policy` and `v` ave the correct data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9968c2af9793e7434f8ee78f14cec855",
     "grade": false,
     "grade_id": "cell-a6c6aae6c77a71fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is your 'policy':\n",
      "['n' 'n' 'n' 'n' 'n' 'n' 'n' 'n' 'n' 'n' 'n' 'n' 'n' 'e' 'n' 'n' 'n' 'n'\n",
      " 'n' 'n' 'e' 'e' 'e' 'n' 'w']\n",
      "These are your state values 'v':\n",
      "[3. 4. 5. 4. 5. 4. 5. 6. 5. 6. 5. 6. 7. 6. 7. 6. 7. 8. 0. 8. 7. 8. 9. 0.\n",
      " 9.]\n"
     ]
    }
   ],
   "source": [
    "# Print the values you computed\n",
    "print(\"This is your 'policy':\")\n",
    "print(policy)\n",
    "print(\"These are your state values 'v':\")\n",
    "print(v)\n",
    "\n",
    "# Check whether both policy and v are numpy arrays.\n",
    "import numpy as np\n",
    "assert(isinstance(policy, np.ndarray))\n",
    "assert(isinstance(v, np.ndarray))\n",
    "\n",
    "# Check correct shapes of numpy arrays.\n",
    "assert(policy.shape == (25, ))\n",
    "assert(v.shape == (25, ))\n",
    "\n",
    "# Check whether the numpy arrays have the correct data types.\n",
    "assert(np.issubdtype(policy.dtype, np.unicode_)) # policy.dtype should be '<U1'\n",
    "assert(np.issubdtype(v.dtype, np.float64))\n",
    "\n",
    "# Check whether all policy values are either \"n\", \"w\", \"s\", or \"e\".\n",
    "assert(np.all(np.isin(policy, np.array([\"n\", \"w\", \"s\", \"e\"])))) \n",
    "\n",
    "# Arrays with CORRECT data types (but WRONG values!) would be, for example:\n",
    "# policy = np.array([\"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \n",
    "#                    \"n\", \"w\", \"s\", \"w\", \"e\", \"n\", \"w\", \"s\", \"w\", \"e\", \n",
    "#                    \"n\", \"w\", \"s\", \"w\", \"e\"])\n",
    "# v = np.random.rand(25)\n",
    "# DO NOT UNCOMMENT THE PREVIOUS lines... otherwise they will overwrite the arrays that you computed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ea83e5f666928d35763bae9674572081",
     "grade": true,
     "grade_id": "cell-68e298725adc680d",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL. \n",
    "# Your code for Exercise 1 is tested here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Stochastic environment\n",
    "\n",
    "In this second exercise, the agent is not always able to execute its actions as intended. With probability 0.8, it moves in the intended direction. With probability 0.2, it moves in a random direction. For example, from grid square 0, if the agent executes action _north_, with probability 0.8, the action will work as intended. But with probability 0.2, the agent's motor control system will move in a random direction (including north). For example, with probability 0.05, it will try to move west (where it will be blocked by the wall and hence remain in grid square 0). Notice that the total probability of moving to square 5 (as intended) is 0.8 + 0.05 = 0.85.\n",
    " \n",
    "Compute the optimal policy using Value Iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "dc139274696552f27c3dcac36ce740ff",
     "grade": false,
     "grade_id": "cell-012cf351d77ce75d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6.04169329 7.28756636 8.61359951 0.         8.69262311]\n",
      " [4.86185111 5.99087587 6.37082431 0.         6.46721593]\n",
      " [3.67550938 4.69621388 4.99441863 3.2189158  5.10250988]\n",
      " [2.48699534 3.40945989 3.66922967 2.64122933 3.78610115]\n",
      " [1.35979208 2.19733672 2.42878751 1.57272161 2.55202451]]\n",
      "[['e' 'e' 'e' 'n' 'w']\n",
      " ['n' 'n' 'n' 'n' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'e' 'n']\n",
      " ['n' 'n' 'n' 'n' 'n']]\n"
     ]
    }
   ],
   "source": [
    "def playStoch(environment, episodes=500):\n",
    "    \n",
    "    \n",
    "    stoch = vcalc(environment)\n",
    "    for episode in range(0, episodes):\n",
    "        environment.reset()\n",
    "    \n",
    "        #loop through all states\n",
    "        for i in range(environment.num_fields):\n",
    "            state = i\n",
    "            \n",
    "            #find next positions for state\n",
    "            positions = environment.availablePositions(state)\n",
    "            \n",
    "            #get v and policy tables\n",
    "\n",
    "            vtabstoch, bestpolstoch = stoch.buildValTableStoch(state, positions, environment)\n",
    "\n",
    "\n",
    "    return vtabstoch, bestpolstoch\n",
    "\n",
    "v,policy = playStoch(environment, episodes=500)\n",
    "\n",
    "print(np.fliplr(v[::-1].reshape(5,5)))\n",
    "\n",
    "print(np.fliplr(policy[::-1].reshape(5,5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79c5cffff4eeffd21c5590981f487de5",
     "grade": true,
     "grade_id": "cell-ff4a8be47847a6a9",
     "locked": true,
     "points": 4,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell. Your code for Exercise 2 is tested here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Exercise 3: Stochastic environment with two pieces of gold (3 marks)\n",
    "\n",
    "<img src=\"images/bomb and two gold.png\" style=\"width: 300px;\" align=\"left\" caption=\"Figure 1\"/> In this third exercise, the environment is identical to the environment in Exercise 2 with the following exception: there is an additional piece of gold on grid square 12. Recall from earlier instructions that the terminal state is reached only when _all_ gold is collected or when the bomb is activated.\n",
    "\n",
    "Compute the optimal policy using Value Iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gridworld1:\n",
    "    def __init__(self):\n",
    "        self.num_rows = 5\n",
    "        self.num_cols = 5\n",
    "        self.num_grids = 3\n",
    "\n",
    "        self.gold_reward = 10\n",
    "        self.bomb_reward = -10\n",
    "        \n",
    "        self.gold_positions = np.array([12,23,37,73])\n",
    "        \n",
    "        self.gold_positions_terminal = np.array([37,73])\n",
    "        \n",
    "\n",
    "        self.bomb_positions = np.array([18, 43, 68])\n",
    "\n",
    "        \n",
    "\n",
    "        self.actions = [\"UP\", \"RIGHT\", \"DOWN\", \"LEFT\"]\n",
    "        self.num_actions = len(self.actions)\n",
    "        self.num_fields = self.num_cols * self.num_rows * self.num_grids\n",
    "\n",
    "        self.rewardsW1 = np.array([12,23])\n",
    "        self.rewardsw2 = np.array([48])\n",
    "        self.rewardsw3 = np.array([62])\n",
    "        \n",
    "        \n",
    "        self.terminalw2 = np.array([48])\n",
    "        self.terminalw3 = np.array([62])\n",
    "        \n",
    "        \n",
    "        self.step = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "        self.availableMoves = np.zeros(self.num_actions)\n",
    "        self.stochasticMoves = np.zeros(shape=(self.num_fields,self.num_actions))\n",
    "        self.gridNum = 1\n",
    "        self.terminalStates = self.bomb_positions\n",
    "        \n",
    "    def gridSelect(self, state):\n",
    "        if state == self.gold_position1:\n",
    "            self.gridNum == 2\n",
    "            self.terminalStates = np.append(self.terminalStates,self.gold_position4)\n",
    "            newstate = self.gold_position3\n",
    "            return newstate\n",
    "            \n",
    "        elif state == self.gold_position2:\n",
    "            self.gridNum == 3\n",
    "            self.terminalStates=np.append(self.terminalStates,self.gold_position3)\n",
    "            newstate = self.gold_position4\n",
    "            return newstate\n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def getRewards (self, state):\n",
    "        self.rewards = np.zeros(shape=self.num_fields)\n",
    "        self.rewards[self.bomb_positions] = self.bomb_reward\n",
    "        \n",
    "        if state < 25:\n",
    "            self.rewards[self.rewardsW1] = self.gold_reward\n",
    "        elif state < 50:\n",
    "            self.rewards[self.rewardsw2] = self.gold_reward\n",
    "        else:\n",
    "            self.rewards[self.rewardsw3] = self.gold_reward\n",
    "        return self.rewards\n",
    "    \n",
    "    def terminal (self, state):\n",
    "        \n",
    "        self.terminalStates = np.append(self.bomb_positions,self.terminalw2)\n",
    "        \n",
    "        \n",
    "        self.terminalStates = np.append(self.terminalStates, self.terminalw3)\n",
    "    \n",
    "        return self.terminalStates\n",
    "        \n",
    "    \n",
    "    def availablePositions(self, oldstate):\n",
    "        # Determine new available positions and check whether the agent hits a wall.\n",
    "        #[up,right,down,left]\n",
    "    \n",
    "        #check world and make sure dont transition between worlds\n",
    "        \n",
    "        if oldstate < 25:\n",
    "            world = 1\n",
    "            upper = 25\n",
    "            lower = 0\n",
    "        elif oldstate < 50:\n",
    "            world = 2\n",
    "            upper = 50\n",
    "            lower = 25\n",
    "        else:\n",
    "            world =3\n",
    "            upper = 75\n",
    "            lower = 50\n",
    "        \n",
    "        #up\n",
    "        candidate_position = oldstate + self.num_cols\n",
    "        if candidate_position < upper:\n",
    "            self.availableMoves[0] = candidate_position\n",
    "        else:\n",
    "            self.availableMoves[0] = oldstate\n",
    "        \n",
    "        #right\n",
    "        candidate_position = oldstate + 1\n",
    "        if candidate_position % self.num_cols > 0:\n",
    "            self.availableMoves[1] = candidate_position\n",
    "        else:\n",
    "            self.availableMoves[1] = oldstate\n",
    "        \n",
    "        #down\n",
    "        candidate_position = oldstate - self.num_cols\n",
    "        if candidate_position >= lower:\n",
    "            self.availableMoves[2] = candidate_position\n",
    "        else:\n",
    "            self.availableMoves[2] = oldstate\n",
    "        \n",
    "        #left\n",
    "        candidate_position = oldstate - 1\n",
    "        if candidate_position % self.num_cols < self.num_cols - 1:\n",
    "            self.availableMoves[3] = candidate_position\n",
    "        else:\n",
    "            self.availableMoves[3] = oldstate\n",
    "        \n",
    "        \n",
    "        return self.availableMoves\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = np.random.randint(0, 5)\n",
    "\n",
    "    def is_terminal_state(self):\n",
    "        # The following statement returns a boolean. It is 'True' when the agent_position\n",
    "        # coincides with any bomb_positions or gold_positions.\n",
    "        #return np.append(self.bomb_positions, self.gold_positions)\n",
    "        return np.append(self.bomb_positions, self.gold_positions_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vcalc1:\n",
    "    def __init__(self, environment, epsilon=0.05, alpha=0.1, gamma=1):\n",
    "        self.environment = environment\n",
    "\n",
    "        self.valtable = np.zeros(shape=(self.environment.num_fields))\n",
    "        self.transProb = np.zeros(shape=(self.environment.num_fields, self.environment.num_actions,self.environment.num_fields))\n",
    "        self.bestPolicy = np.empty(shape=(self.environment.num_fields),dtype=str)\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def buildValTableStoch(self,state,positions, environment):      \n",
    "        \n",
    "        positions = [int(x) for x in positions]\n",
    "        \n",
    "        reward = np.empty(len(positions))\n",
    "        for i in range(len(positions)):\n",
    "            \n",
    "            reward[i] = environment.getRewards(state)[int(positions[i])]-1\n",
    "            \n",
    "        terminal = environment.terminal(state)\n",
    "        \n",
    "        #check if terminal state\n",
    "        if state in terminal:\n",
    "            \n",
    "            self.valtable[state] = 0\n",
    "            \n",
    "            #so passes assessment check\n",
    "            self.bestPolicy[state] = 'n'\n",
    "            \n",
    "        else:\n",
    "            availableVs = self.valtable[positions]\n",
    "\n",
    "            reward_x_v = reward + availableVs\n",
    "                        \n",
    "        #calculate stochatic v values for each position\n",
    "            stochV = np.zeros(4)\n",
    "            \n",
    "            stochV[0] = 0.85*reward_x_v[0] + 0.05*reward_x_v[1] + 0.05*reward_x_v[2] +0.05*reward_x_v[3]\n",
    "\n",
    "            stochV[1] = 0.05*reward_x_v[0] + 0.85*reward_x_v[1] + 0.05*reward_x_v[2] +0.05*reward_x_v[3]\n",
    "\n",
    "            stochV[2] = 0.05*reward_x_v[0] + 0.05*reward_x_v[1] + 0.85*reward_x_v[2] +0.05*reward_x_v[3]\n",
    "\n",
    "            stochV[3] = 0.05*reward_x_v[0] + 0.05*reward_x_v[1] + 0.05*reward_x_v[2] +0.85*reward_x_v[3]\n",
    "            \n",
    "        \n",
    "            stochV = np.array(stochV)\n",
    "            \n",
    "            max_v_value = np.max(stochV)\n",
    "            \n",
    "            \n",
    "            max_v_pos_list = list(np.nonzero(stochV == max_v_value))[0]\n",
    "            \n",
    "            #calculate v table and pos table\n",
    "            \n",
    "            max_v_pos = random.choice(max_v_pos_list)\n",
    "\n",
    "            \n",
    "            policies = ['n','e','s','w']\n",
    "            \n",
    "            self.valtable[state] = max_v_value\n",
    "            self.bestPolicy[state] = policies[max_v_pos]\n",
    "\n",
    "        \n",
    "        return self.valtable,self.bestPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play2gold(environment, episodes=500):\n",
    "    \n",
    "\n",
    "    stoch = vcalc1(environment)\n",
    "    for episode in range(0, episodes):\n",
    "        environment.reset()\n",
    "\n",
    "        #while step < max_steps_per_episode and not game_over:\n",
    "        for state in range(environment.num_fields):\n",
    "            #if one gold selected move to next world\n",
    "            if state == 12:\n",
    "                positions = environment.availablePositions(37)\n",
    "            elif state == 23: \n",
    "                positions = environment.availablePositions(73)\n",
    "            else:\n",
    "                positions = environment.availablePositions(state)\n",
    "            \n",
    "\n",
    "            vtabstoch, bestpolstoch = stoch.buildValTableStoch(state, positions, environment)\n",
    "\n",
    "\n",
    "    return vtabstoch, bestpolstoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "646247d5d78f4d5a40f48bee98eae88c",
     "grade": false,
     "grade_id": "cell-0a694ce4c1e1c78e",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['e' 'e' 'e' 'w' 'w']\n",
      " ['e' 's' 's' 'n' 'n']\n",
      " ['e' 'e' 'n' 'w' 'w']\n",
      " ['e' 'n' 'n' 'w' 'w']\n",
      " ['n' 'n' 'n' 'n' 'w']]\n",
      "[[10.65103994 11.79603433 13.00848756  4.28547547 12.97048714]\n",
      " [11.1861353  12.32932372 12.5121464   0.         10.61568556]\n",
      " [12.28702748 13.59365638  4.99441863 12.41930416 11.19974428]\n",
      " [11.17522837 12.35165962 13.59092292 12.28122217 11.051285  ]\n",
      " [10.06409806 11.17488271 12.28045984 11.10816476  9.99389366]]\n"
     ]
    }
   ],
   "source": [
    "# Please write your code for Exercise 3 here. Your code should compute the \n",
    "# values of policy and v from scratch when this cell is excuted, using the value \n",
    "# iteration algorithm. We will mark your coursework by checking the values of \n",
    "# the variables policy and v in the following cell. \n",
    "environment = Gridworld1()\n",
    "\n",
    "\n",
    "v,policy = play2gold(environment, episodes=500)\n",
    "\n",
    "\n",
    "policy = policy[:25]\n",
    "\n",
    "v = v[:25]\n",
    "\n",
    "\n",
    "print(np.fliplr(policy[::-1].reshape(5,5)))\n",
    "\n",
    "print(np.fliplr(v[::-1].reshape(5,5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "795410ef7038c03ef45dedfa9332d6f5",
     "grade": true,
     "grade_id": "cell-55c5920e89b768cf",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not delete this cell. Your code for Exercise 3 is tested here.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
