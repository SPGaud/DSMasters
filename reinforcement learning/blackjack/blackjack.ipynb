{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blackjack\n",
    "\n",
    "Here we will implement **Monte Carlo Policy Evaluation (MCPE)** to learn the state-value function $V(s)$ for a given policy in the game of [blackjack](https://en.wikipedia.org/wiki/Blackjack).\n",
    "\n",
    "### The game\n",
    "\n",
    "**Rules.** We will use the version of the game discussed in the lectures where a single player (the agent) plays against the dealer. The player's objective is to obtain cards whose sum is as large as possible without exceeding 21. All face cards count as 10; an ace can count as either 1 or 11.\n",
    "\n",
    "The game begins with two cards dealt to both the dealer and the player. The first of the dealer’s cards is face down and the second is face up. If the player has 21 immediately (for example, an ace and a face card), it is called a \"blackjack\". The player then wins unless the dealer also has a blackjack, in which case the game is a draw. If the player does not have a blackjack, then she can request additional cards, one by one (_hits_), until she either stops (_sticks_) or exceeds 21 (_goes bust_). If the player goes bust, she loses; if she sticks, then it becomes the dealer’s turn. \n",
    "\n",
    "The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, hits otherwise. If the dealer goes bust, then the player wins; otherwise, the outcome (win, lose, or draw) is determined by whose final sum is closer to 21.\n",
    "\n",
    "**MDP formulation.** Playing blackjack is naturally formulated as an episodic finite MDP. Each game of blackjack is an episode. Rewards of +1, −1, and 0 are given for winning, losing, and drawing, respectively. All rewards until the end of the game are zero. We do not discount ($\\gamma = 1$); therefore these terminal rewards are also the returns. The player’s actions are to `\"hit\"` or to `\"stick\"`. \n",
    "\n",
    "The states depend on the player’s cards and the dealer’s showing card. Assume that cards are dealt from an infinite deck (that is, with replacement) so that there is no advantage to keeping track of the cards already dealt. If the player holds an ace that she could count as 11 without going bust, then the ace is said to be _usable_. In this case it is always counted as 11 because counting it as 1 would make the sum 11 or less, in which case there is no decision to be made because, obviously, the player should always hit. Thus, the player makes decisions on the basis of three variables: \n",
    "- the player's current sum (an integer between 12 and 21);\n",
    "- the dealer’s one showing card (an integer between 1 and 10; note that the ace is counted as 1 here); and\n",
    "- whether or not the player holds a usable ace (a boolean). \n",
    "\n",
    "This makes for a total of 200 states. We represent the state as a numpy-array of length 3 that combines the just mentioned three variables in the given order. For example, if the player is given a 6 and a _jack_, and the dealer's showing card is an ace, the corresponding state will be the numpy array `[16, 1, False]`. The terminal state of the game will be denoted by the numpy array `[-1, -1, -1]`.\n",
    "\n",
    "We use a `Blackjack` class to simulate the blackjack games.\n",
    "\n",
    "We import the blackjack module and create a blackjack environment called `env`. The constructor method has one argument called `verbose`. If `verbose=True`, the blackjack object will regularly print the progress of the game. This is useful for getting to know the game and the provided code or if you just want to play around. You may want to set `verbose=False` when you run thousands of episodes to complete the exercise below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import blackjack\n",
    "env = blackjack.Blackjack(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can interact with the blackjack environment using the `make_step()` method. This method takes an `action` as input and computes the response of the environment. Specifically, this method returns the resulting `new_state` and the corresponding `reward` signal.\n",
    "\n",
    "Before the player can perform actions, we have to start the game (e.g., draw starting hands). In order to start or reset a blackjack game, call the `make_step()` without specifying a specific action or by setting `action=\"reset\"`.\n",
    "\n",
    "We will now walk through several example games. We will specify a [random seed](https://en.wikipedia.org/wiki/Random_seed) for the NumPy pseudo random number generator every time before we reset the game. This allows us to keep these examples reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The game is reset.\n",
      "Player's cards: [10, 10]\n",
      "Dealer's showing card: [7]\n",
      "Initial state: [20  7  0]\n",
      "Reward: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(8)\n",
    "new_state, reward = env.make_step(action=\"reset\")\n",
    "print(\"Initial state:\", new_state)\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The player drew two cards with face value 10 each. The dealer also drew two cards, but we can only see the second card, a 7. The player now can choose to \"hit\" or \"stick\". Most players would stick if they had 20 on their hand. We call again the `make_step()` method and specify `action=\"stick\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dealer's cards are: [10, 7]\n",
      "The dealer has 17 points.\n",
      "PLAYER WINS!\n",
      "The player obtains a reward of 1\n",
      "The new (terminal) state is: [-1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "new_state, reward = env.make_step(action = \"stick\")\n",
    "print(\"The player obtains a reward of\", reward)\n",
    "print(\"The new (terminal) state is:\", new_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The player won and received a reward of 1. Whenever an episode ends, the environment object sets the internal variable `self.active` to `False`. This variable is set to `True` again when we _reset_ the game. You can use the `self.active` variable to check whether an episode has ended or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The game is reset.\n",
      "Player's cards: [11, 7]\n",
      "Dealer's showing card: [2]\n",
      "New state: [18  2  1]\n",
      "reward 0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(9)\n",
    "new_state, reward = env.make_step(action=\"reset\")\n",
    "print(\"New state:\", new_state)\n",
    "print('reward',reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The player has already 18 points but has a _usable ace_, which she can transfer into a 1 whenever she would _go bust_. The player can thus \"hit\" and hope that she gets closer to 21. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player draws card: [2]\n",
      "New sum of player's cards: [20]\n",
      "New state: [20  2  1]\n",
      "reward 0\n"
     ]
    }
   ],
   "source": [
    "new_state, reward = env.make_step(action = \"hit\")\n",
    "print(\"New state:\", new_state)\n",
    "print('reward',reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! The player got another 2 points and has again 20 points. The player would probably want to \"stick\" again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dealer's cards are: [7, 2]\n",
      "The dealer has 9 points.\n",
      "Dealer draws card: [3]\n",
      "New dealer sum [12]\n",
      "Dealer draws card: [6]\n",
      "New dealer sum [18]\n",
      "PLAYER WINS!\n",
      "New state: [-1 -1 -1]\n",
      "reward 1\n"
     ]
    }
   ],
   "source": [
    "new_state, reward = env.make_step(action = \"stick\")\n",
    "print(\"New state:\", new_state)\n",
    "print('reward',reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The player won again! Let's play a last one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The game is reset.\n",
      "Player's cards: [10, 11]\n",
      "Dealer's showing card: [10]\n",
      "Player has Blackjack!\n",
      "The dealer's cards are: [9, 10]\n",
      "PLAYER WINS!\n",
      "The game is reset.\n",
      "Player's cards: [7, 3, 3]\n",
      "Dealer's showing card: [3]\n",
      "New state: [-1 -1 -1]\n",
      "Reward: 1\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "new_state, reward = env.make_step()\n",
    "env.make_step()\n",
    "print(\"New state:\", new_state)\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The player drew a \"Blackjack\", that is, an ace and a 10. The dealer's cards valued 16. The player won again and received a reward without having performed an action. Try out some more games to get familiar with the code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to learn the state-value function for the policy **\"Stick if the player's sum is 19 or higher, and hit otherwise.\"**. We will compute these state values using **Monte Carlo Policy Evaluation (MCPE)**. The pseudo-code for MCPE is reproduced below from the textbook (Reinforcement Learning, Sutton & Barto, 1998, Section 5.1).\n",
    "<img src=\"images/MCPE.png\" style=\"width: 400px;\"/>\n",
    "The provided pseudo-code shows _first-visit_ MCPE. No state occurs twice during one game (episode) of Blackjack. In this case, first-visit MCPE and every-visit MCPE are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7051cb2b547a6f02d56440d85ed9d198",
     "grade": false,
     "grade_id": "cell-a46757fc05631afe",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# This cell computes the state values 'v' using MCPE.\n",
    "\n",
    "sim = 1\n",
    "\n",
    "env = blackjack.Blackjack(verbose=False)\n",
    "\n",
    "#values of states outcomes stored in dictionary\n",
    "\n",
    "valDict = {}\n",
    "\n",
    "np.random.seed(7)\n",
    "while sim < 100000:\n",
    "    \n",
    "    #list of all states in game\n",
    "    stateList = []\n",
    "    new_state, reward = env.make_step(action=\"reset\")\n",
    "    \n",
    "    \n",
    "    #game ends on first go\n",
    "    if np.all(new_state == np.array([-1 -1 -1])):\n",
    "        \n",
    "        #continue\n",
    "        gameReward = reward\n",
    "    else:\n",
    "        \n",
    "        #loop while hand less than 19 and game not over\n",
    "        while new_state[0] <19 and new_state[0] !=-1:\n",
    "            \n",
    "            #store all states\n",
    "            stateList.append(new_state)\n",
    "            \n",
    "            new_state, reward = env.make_step(action=\"hit\")\n",
    "            \n",
    "        if new_state[0] >= 19 and new_state[0] !=-1:\n",
    "            \n",
    "            #stick if over 19\n",
    "            \n",
    "            stateList.append(new_state)\n",
    "            new_state, reward = env.make_step(action=\"stick\")\n",
    "            \n",
    "            #reward of game\n",
    "            gameReward = reward\n",
    "            \n",
    "        if new_state[0] == -1:\n",
    "            \n",
    "            gameReward = reward\n",
    "            \n",
    "            #store all result of game to each state and add \n",
    "            for state in stateList:\n",
    "                \n",
    "                if str(state) in valDict:\n",
    "                    sumRewards = valDict[str(state)][0] + gameReward\n",
    "                    \n",
    "                    #total number of times state has been acheived                \n",
    "                    totalTimes = valDict[str(state)][1] + 1\n",
    "                    valDict[str(state)] = [sumRewards,totalTimes]\n",
    "                else:\n",
    "                    valDict[str(state)] = [gameReward,1]\n",
    "            \n",
    "            \n",
    "        \n",
    "        sim += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2795f1f5b1ad7d61734a05171ab66724",
     "grade": false,
     "grade_id": "cell-cddecbc6ed38c8c8",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "#  return monte carlo policy evaluation of state\n",
    "def get_state_value(s, v):\n",
    "    s = np.array(s)\n",
    "    value_of_s = float(v[str(s)][0])/float(v[str(s)][1])\n",
    "        \n",
    "    return value_of_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6442792036459583\n"
     ]
    }
   ],
   "source": [
    "print(get_state_value([17,10,0],valDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "05452803387d8ef53bea82b994da9598",
     "grade": true,
     "grade_id": "cell-3345547d747d76e4",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. We will use it to mark your solution. \n",
    "# All of your code must be written above this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a8c1f0ee7cd16952c6b2bf5044d756a8",
     "grade": true,
     "grade_id": "cell-dde38a88e3c2472d",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "79658d5a7137ead20289454dc56d1649",
     "grade": true,
     "grade_id": "cell-a80faee7205f5ad6",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "380294c069cd68447f27bc8cae755ea4",
     "grade": true,
     "grade_id": "cell-359c2bf96e98a580",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4a2acbf1500ae5f0e5d860d9fe16dce9",
     "grade": true,
     "grade_id": "cell-f31a3e65fcf60bd4",
     "locked": true,
     "points": 20,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This is a TEST CELL. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
