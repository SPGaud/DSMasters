{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Information Extraction\n",
    "\n",
    "\n",
    "Here will be implementing an *open information extraction* system, almost entirely from scratch. Information extraction takes a body of freeform text and extracts the contained information in a computer interpretable form. The word *open* simply means that the text/facts are arbitrary, so it will work with any input rather than a specific domain (e.g. legal texts).\n",
    "\n",
    "As an example, given the input:\n",
    "\n",
    "> \"Trolls really don't like the sun.\"\n",
    "  \n",
    "\n",
    "you may extract the \"fact\":\n",
    "```\n",
    "('Trolls', 'do not like', 'the sun')\n",
    "```\n",
    "\n",
    "The approach is based on the paper \"*Identifying Relations for Open Information Extraction*\", by Fader, Soderland & Etzioni.\n",
    "\n",
    "The steps of the system are as follows:\n",
    "*  Tokenise and split on sentences\n",
    "1. Part of speech tagging - token level\n",
    "2. Part of speech tagging - sentence level\n",
    "3. Named entity resolution\n",
    "4. Relation extraction\n",
    "\n",
    "\n",
    "*  Summarise \"*20,000 leagues under the seas*\" by Jules Verne *(provided)*\n",
    "\n",
    "A simple NLP library, called `ogonek`, is used. It has some basic functionality that we will require\n",
    "\n",
    "Its documentation can be found below in a markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ogonek\n",
    "\n",
    "A tiny NLP library, that contains exactly the functionality I don't want you to implement for this coursework!\n",
    "\n",
    "\n",
    "\n",
    "### Tokenisation and sentence splitting\n",
    "`ogonek.Tokenise()`\n",
    "\n",
    "A class that tokenises some text and splits it into sentences. Construct an instance with `tokens = ogonek.Tokenise('My text')`; it then has the same interface as a list of lists:\n",
    "* `len(tokens)`: Number of extracted sentences (not words)\n",
    "* `tokens[i]`: Sentence i, where i ranges from 0 to one less than `len(tokens)`. A sentence is a list of tokens.\n",
    "\n",
    "\n",
    "\n",
    "### Word vectors\n",
    "`ogonek.Glove()`\n",
    "\n",
    "Constructing a `glove = ogonek.Glove()` object loads a heavily pruned Glove word vectors from the file `baby_glove.zip` into memory, and will then translate tokens into word vectors. Note that it automatically lowercases any token it is handed, so you don't need to. Has the following interface:\n",
    "* `glove.len_vec()` - Returns the length of the word vectors; should be 300.\n",
    "* `len(glove)` - Returns how many word vectors it knows of.\n",
    "* `token in glove` - Returns `True` if it has a word vector for that token, `False` otherwise.\n",
    "* `glove[token]` - Returns the word vector for the given token; raises an error if it does not have one.\n",
    "* `glove.decode(token)` - Returns the word vector for the given token, but if the word vector is unknown returns a vector of zeros instead (silent failure).\n",
    "* `glove.decodes(list of tokens)` - Returns a list of word vectors, one for each token. Has the same silent failure behaviour as `decode`.\n",
    "\n",
    "\n",
    "\n",
    "### Groningen Meaning Bank dataset\n",
    "`ogonek.GMB()`\n",
    "\n",
    "Provides access to the Groningen Meaning Bank dataset, which is supplied in the file `ner_dataset.csv`. Replicates the interface of the tokenisation system as far as it can. Construct with `gmb = ogonek.GMB()`; has the following interface:\n",
    "* `len(gmb)`: Number of sentences (not words) in data set\n",
    "* `gmb[i]`: Sentence i, where i ranges from 0 to one less than `len(gmb)`. A sentence is a list of tokens.\n",
    "* `gmb.pos(i)`: A list of POS tags that match with sentence i. Note that these are the full Penn Treebank tags (not the reduced set used below).\n",
    "* `gmb.ner(i)`: A list of named entities that match with sentence i. Using outside-inside scheme.\n",
    "\n",
    "\n",
    "\n",
    "### Pretty printing\n",
    "\n",
    "`ogonek.aligned_print(*)` takes multiple lists and prints them out, aligning them so that all elements in position 0 of all lists are aligned vertically (extra space added as required), and then elements in position 1 and so on. For showing tags and a sentence with everything aligned. Also does word wrap and colour coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import string\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "import ogonek\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = ogonek.GMB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary giving descriptions of the reduced part of speech tags...\n",
    "rpos_desc = {'C' : 'Coordinating conjunction',\n",
    "             '0' : 'Cardinal number',\n",
    "             'D' : 'Determiner',\n",
    "             'E' : 'Existential there',\n",
    "             'I' : 'Preposition or subordinating conjunction',\n",
    "             'J' : 'Adjective',\n",
    "             'N' : 'Noun',\n",
    "             'P' : 'Predeterminer',\n",
    "             'S' : 'Possessive ending',\n",
    "             'M' : 'Pronoun',\n",
    "             'R' : 'Adverb',\n",
    "             'Z' : 'Particle',\n",
    "             'T' : 'to',\n",
    "             'V' : 'Verb',\n",
    "             'A' : 'Anything else',\n",
    "             '.' : 'All punctuation'}\n",
    "\n",
    "\n",
    "\n",
    "# Reduced list of part of speech tags as a list...\n",
    "num_to_rpos = ['C', '0', 'D', 'E', 'I', 'J', 'N', 'P',\n",
    "               'S', 'M', 'R', 'Z', 'T', 'V', 'A', '.']\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary that maps a reduced part of speech\n",
    "# tag to it's index in the above list; useful for vectors/matrices etc...\n",
    "rpos_to_num = {'C' : 0,\n",
    "               '0' : 1,\n",
    "               'D' : 2,\n",
    "               'E' : 3,\n",
    "               'I' : 4,\n",
    "               'J' : 5,\n",
    "               'N' : 6,\n",
    "               'P' : 7,\n",
    "               'S' : 8,\n",
    "               'M' : 9,\n",
    "               'R' : 10,\n",
    "               'Z' : 11,\n",
    "               'T' : 12,\n",
    "               'V' : 13,\n",
    "               'A' : 14,\n",
    "               '.' : 15}\n",
    "\n",
    "\n",
    "\n",
    "# Dictionary that maps the full part of speech tags to the reduced set...\n",
    "pos_to_rpos = {'CC' : 'C',\n",
    "               'CD' : '0',\n",
    "               'DT' : 'D',\n",
    "               'EX' : 'E',\n",
    "               'FW' : 'A',\n",
    "               'IN' : 'I',\n",
    "               'JJ' : 'J',\n",
    "               'JJR' : 'J',\n",
    "               'JJS' : 'J',\n",
    "               'LS' : 'A',\n",
    "               'MD' : 'A',\n",
    "               'NN' : 'N',\n",
    "               'NNS' : 'N',\n",
    "               'NNP' : 'N',\n",
    "               'NNPS' : 'N',\n",
    "               'PDT' : 'P',\n",
    "               'POS' : 'S',\n",
    "               'PRP' : 'M',\n",
    "               'PRP$' : 'M',\n",
    "               'RB' : 'R',\n",
    "               'RBR' : 'R',\n",
    "               'RBS' : 'R',\n",
    "               'RP' : 'Z',\n",
    "               'SYM' : 'A',\n",
    "               'TO' : 'T',\n",
    "               'UH' : 'A',\n",
    "               'VB' : 'V',\n",
    "               'VBD' : 'V',\n",
    "               'VBG' : 'V',\n",
    "               'VBN' : 'V',\n",
    "               'VBP' : 'V',\n",
    "               'VBZ' : 'V',\n",
    "               'WDT' : 'D',\n",
    "               'WP' : 'M',\n",
    "               'WP$' : 'S',\n",
    "               'WRB' : 'R',\n",
    "               '-' : '.',\n",
    "               'LRB' : '.',\n",
    "               'RRB' : '.',\n",
    "               '``' : '.',\n",
    "               '\"' : '.',\n",
    "               '.' : '.',\n",
    "               ',' : '.',\n",
    "               ';' : '.',\n",
    "               ':' : '.',\n",
    "               '$' : '.'}    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load book, tokenise and split on sentences\n",
    "The below code reads in the book, chops it down to just the text of the book, and then tokenises it using the provided `ogonek` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01. It was the regime of verticality .\n",
      "02. Now then , the tides are not strong in the Pacific , and if you can not unballast the Nautilus , which seems impossible to me , I do not see how it will float off . \"\n",
      "03. Captain Nemo left the cave , and we climbed back up the bank of shellfish in the midst of these clear waters not yet disturbed by divers at work .\n",
      "04. Likewise the pilothouse and the beacon housing were withdrawn into the hull until they lay exactly flush with it .\n",
      "05. Instead of digging all around the Nautilus , which would have entailed even greater difficulties , Captain Nemo had an immense trench outlined on the ice , eight meters from our port quarter .\n",
      "06. We would not go five miles without bumping into a fellow countryman .\n",
      "07. The oars , mast , and sail are in the skiff .\n",
      "08. Under existing conditions some ten men at the most should be enough to operate it . \"\n",
      "09. Nobody appeared on our arrival .\n",
      "10. We gasped .\n"
     ]
    }
   ],
   "source": [
    "# Loop file, only keeping lines between indicators...\n",
    "lines = []\n",
    "record = False\n",
    "\n",
    "with open('20,000 Leagues Under the Seas.txt', 'r', encoding='utf8') as fin:\n",
    "    for line in fin:\n",
    "        if record:\n",
    "            if line.startswith('***END OF THE PROJECT GUTENBERG'):\n",
    "                break\n",
    "      \n",
    "            lines.append(line)\n",
    "    \n",
    "        else:\n",
    "            if line.startswith('***START OF THE PROJECT GUTENBERG'):\n",
    "                record = True\n",
    "\n",
    "text = ''.join(lines)\n",
    "\n",
    "\n",
    "# Tokenise...\n",
    "under_the_seas = ogonek.Tokenise(text)\n",
    "\n",
    "\n",
    "# Print 10 random sentences to check it worked...\n",
    "numpy.random.seed(0)\n",
    "\n",
    "for i in range(10):\n",
    "    toks = numpy.random.choice(under_the_seas)\n",
    "    print('{:02d}. {}'.format(i+1, ' '.join(toks)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging - token level\n",
    "\n",
    "The goal here is to train a classifier that indicates which of the part of speech tags (the reduced set provided above) each word is. For this initial approach we're going to treat words (tokens) individually, without context. For features the Glove word vectors are going to be used (provided by `ogonek.Glove()`).\n",
    "\n",
    "Instead of training a single classifier a slight modification of a random kitchen sink for each part of speech tag is going to be used. Specifically, a logistic random kitchen sink that indicates the probability that the word should be labelled with the associated tag. This is a *one vs all* classifier - you have a classifier for every tag, run them all on each word, and then select the tag with the highest probability (it's inconsistent - they won't sum to 1!). A logistic random kitchen sink is simply a normal kitchen sink that is pushed through a sigmoid function (in neural network terms, the final layer has a non-linearity),\n",
    "$$\\operatorname{Sig}(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "such that the final binary classifier is\n",
    "$$P(\\textrm{tag}) = \\operatorname{Sig}\\left(\\sum_{k \\in K} \\alpha_k \\phi\\left(\\vec{x} \\cdot \\vec{w}_k\\right)\\right)$$\n",
    "For the cost function we will be maximising the log likelihood of the dataset. This will require gradient descent; Nestorov, including backtracking line search to select the initial step size, to get all marks. We will be using 300 random features, in addition to the 300 provided by glove (total of 601 - bias term is the +1), as that keeps the resulting data matrix during training small enough that it completes reasonably quickly.\n",
    "\n",
    "The Groningen Meaning Bank dataset has been provided; it can be accessed via the class `ogonek.GMB`. It includes lots of sentences, each as a list of tokens, plus part of speech tags as a list aligned with the sentence.\n",
    "Source: https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMB sentences = 47959\n",
      "\n",
      "\u001b[0mThe U.S.  space agency is  making final preparations to launch the first \n",
      "\u001b[31mDT  NNP   NN    NN     VBZ VBG    JJ    NNS          TO VB     DT  JJ    \n",
      "\u001b[34mO   B-geo O     O      O   O      O     O            O  O      O   O     \n",
      "\u001b[0m\n",
      "\u001b[0mdirect space probe to the distant planet of Pluto . \n",
      "\u001b[31mJJ     NN    NN    TO DT  JJ      NN     IN NNP   . \n",
      "\u001b[34mO      O     O     O  O   O       O      O  B-geo O \n",
      "\u001b[0m\n",
      "\u001b[0mOn Monday , the freighter Torgelow was hijacked off the eastern coast of \n",
      "\u001b[31mIN NNP    , DT  NN        NNP      VBD VBN      IN  DT  JJ      NN    IN \n",
      "\u001b[34mO  B-tim  O O   O         B-art    O   O        O   O   O       O     O  \n",
      "\u001b[0m\n",
      "\u001b[0mSomalia . \n",
      "\u001b[31mNNP     . \n",
      "\u001b[34mB-geo   O \n",
      "\u001b[0m\n",
      "\u001b[0mChile and Bolivia are associate members . \n",
      "\u001b[31mNNP   CC  NNP     VBP JJ        NNS     . \n",
      "\u001b[34mB-gpe O   B-gpe   O   O         O       O \n",
      "\u001b[0m\n",
      "\u001b[0mVenezuela has freed 11 Colombian soldiers who had been detained after entering \n",
      "\u001b[31mNNP       VBZ VBN   CD JJ        NNS      WP  VBD VBN  VBN      IN    VBG      \n",
      "\u001b[34mB-geo     O   O     O  B-gpe     O        O   O   O    O        O     O        \n",
      "\u001b[0m\n",
      "\u001b[0mVenezuelan territory without authorization . \n",
      "\u001b[31mJJ         NN        IN      NN            . \n",
      "\u001b[34mB-gpe      O         O       O             O \n",
      "\u001b[0m\n",
      "\u001b[0mHowever , the closing figure of 12,012 points was below the record level of \n",
      "\u001b[31mRB      , DT  NN      NN     IN CD     NNS    VBD IN    DT  NN     NN    IN \n",
      "\u001b[34mO       O O   O       O      O  O      O      O   O     O   O      O     O  \n",
      "\u001b[0m\n",
      "\u001b[0m12,049 points reached during trading Wednesday . \n",
      "\u001b[31mCD     NNS    VBN     IN     NN      NNP       . \n",
      "\u001b[34mO      O      O       O      O       B-tim     O \n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load word vectors; in a seperate cell as this takes a couple seconds...\n",
    "glove = ogonek.Glove()\n",
    "\n",
    "\n",
    "# Groningen Meaning Bank dataset - a set of sentences each tagged\n",
    "# with part of speech and named entity recognitiuon tags...\n",
    "gmb = ogonek.GMB()\n",
    "print('GMB sentences = {}'.format(len(gmb)))\n",
    "print()\n",
    "\n",
    "\n",
    "# Print out 5 random sentences from GMB with POS and NER tags, to illustrate the data...\n",
    "numpy.random.seed(1)\n",
    "for _ in range(5):\n",
    "    i = numpy.random.randint(len(gmb))\n",
    "    ogonek.aligned_print(gmb[i], gmb.pos(i), gmb.ner(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create training data\n",
    "def createTrainingSet():\n",
    "    #create list of arrays representing the word vectors of 300 \n",
    "    gloveList = []\n",
    "    #create list of tags (ys)\n",
    "    posWordList = []\n",
    "    sentenceNum = 0\n",
    "    \n",
    "    for sentence in gmb:\n",
    "        sentencePosList = []\n",
    "        sentencePosList = gmb.pos(sentenceNum)\n",
    "        sentenceNum += 1\n",
    "\n",
    "        wordNum = 0\n",
    "        for word in sentence:\n",
    "        #find glove vector for each word\n",
    "            if word in glove:\n",
    "                \n",
    "                gloveList.append(glove[word])\n",
    "\n",
    "                pos = sentencePosList[wordNum]\n",
    "\n",
    "                posWordList.append(rpos_to_num[pos_to_rpos[pos]])\n",
    "                wordNum+=1\n",
    "\n",
    "    return gloveList,posWordList\n",
    "#glovelist corresponds to x\n",
    "#posWordList corresponds to y\n",
    "gloveList,posWordList = createTrainingSet()\n",
    "\n",
    "#create training set as 0.3 of data\n",
    "trainingX = gloveList[:(int(0.3*len(gloveList)))]\n",
    "trainingY = posWordList[:(int(0.3*len(posWordList)))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "#kitchen sink\n",
    "#x is the glovek vector len 300\n",
    "#k is the feature vector len 300 + 1 bias\n",
    "\n",
    "def returnX (x, K = 300): # Extend x with K new features . . . \n",
    "    #create kitchen sink\n",
    "    #create extended x vectors. contain 300 random value vector, 300 dim glove word vector and one bias term\n",
    "    x = np.array(x)\n",
    "    #Random vectors\n",
    "    w = numpy.random.standard_normal((K, x.shape[1])) \n",
    "\n",
    "    nf = numpy.sin(numpy.einsum('ef,gf->eg', x , w))\n",
    "    \n",
    "    #extended vectors\n",
    "    \n",
    "    ex = numpy.append(x,nf, axis=1)\n",
    "    \n",
    "    #add bias term                            \n",
    "    ones = np.ones(x.shape[0])[:,None]\n",
    "    ex = numpy.append(ones,ex, axis=1)\n",
    "\n",
    "    return w, ex\n",
    "\n",
    "\n",
    "                                \n",
    "def prediction(ex, alpha):\n",
    "    #make prediction using alpha\n",
    "    pred = ex@alpha\n",
    "    \n",
    "    pred = np.clip(sigmoid(pred),1e-3,1-1e-3)\n",
    "\n",
    "    return pred\n",
    "                                \n",
    "def costFunction(pred,y): \n",
    "    \n",
    "    #cost function is minimising the negative likelihood \n",
    "    pred = np.array(pred)\n",
    "\n",
    "    cost = -(y@numpy.log(pred)+(1-y)@numpy.log(1-pred))\n",
    "    \n",
    "    return cost\n",
    "\n",
    "def gradient(actual,pred,ex):\n",
    "    #calculate gradient of cost function\n",
    "    diff = (pred-actual)\n",
    "    gradient = ex.T@diff\n",
    "    #(actual - pred)*ex.T\n",
    "    return gradient\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create extended x matrix and return corresponding w\n",
    "w, ex = returnX(trainingX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtrack(ex,y,alpha):\n",
    "    #backtracking algorithm to calculate step size\n",
    "    pred = prediction(ex,alpha)\n",
    "    mu = 0.2\n",
    "    beta = 0.8\n",
    "    #starting step size\n",
    "    step = 0.001\n",
    "    #initalise first alpha\n",
    "    alphaZer = alpha\n",
    "    \n",
    "    #initialised gradients of first alpha\n",
    "    alphaZerGrad = gradient(y,pred,ex)\n",
    "    \n",
    "    #f is cost function\n",
    "\n",
    "    f = lambda alpha:costFunction(prediction(ex,alpha),y)\n",
    "    i = 0\n",
    "    \n",
    "    #keep making step size smaller until it meets condition\n",
    "    while f(alphaZer - step*alphaZerGrad)>=f(alphaZer)-step*mu*numpy.linalg.norm(alphaZerGrad)**2:\n",
    "        step = beta*step\n",
    "        i+= 1\n",
    "    print('step size from back tracking {} '.format(step))\n",
    "    return step\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nesterov(ex, y):\n",
    "    #nesterov gradient descent algorithm\n",
    "    costs = []\n",
    "    #initialise alpha vector\n",
    "    alpha = numpy.random.normal(0, 1, ex.shape[1])\n",
    "    #calculate gradient\n",
    "    delta_f = lambda alpha: gradient(y, prediction(ex, alpha),ex)\n",
    "\n",
    "    # Initialise variables\n",
    "\n",
    "    #initialise step size using\n",
    "    step = backtrack(ex,y,alpha) \n",
    "    \n",
    "#     step = 1e-5\n",
    "    \n",
    "    lam = 0.9\n",
    "\n",
    "    velocity = 0\n",
    "    converged = False\n",
    "    for i in range(256):\n",
    "        # update velocity gradient and use to update alpha\n",
    "        \n",
    "        velocityNew = lam * velocity + step * delta_f(alpha - lam * velocity)\n",
    "        \n",
    "        alpha = alpha - velocityNew\n",
    "        velocity = velocityNew\n",
    "\n",
    "        # Get cost and check convergence\n",
    "        costs.append(costFunction(prediction(ex, alpha),y))\n",
    "        \n",
    "        \n",
    "        #check for convergence\n",
    "        if len(costs) >= 8 and numpy.all(numpy.isclose(costs[-8:], costs[-1])):\n",
    "            converged = True\n",
    "            break\n",
    "\n",
    "    return alpha, costs[-1], prediction(ex, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 14387 sentences for training\n",
      "Training C\n",
      "  (took 77.3458 seconds)\n",
      "Training 0\n",
      "  (took 85.6905 seconds)\n",
      "Training D\n",
      "  (took 76.3827 seconds)\n",
      "Training E\n",
      "  (took 77.4351 seconds)\n",
      "Training I\n",
      "  (took 68.995 seconds)\n",
      "Training J\n",
      "  (took 68.9006 seconds)\n",
      "Training N\n",
      "  (took 69.1477 seconds)\n",
      "Training P\n",
      "  (took 69.7048 seconds)\n",
      "Training S\n",
      "  (took 69.0622 seconds)\n",
      "Training M\n",
      "  (took 68.4559 seconds)\n",
      "Training R\n",
      "  (took 76.0234 seconds)\n",
      "Training Z\n",
      "  (took 71.8655 seconds)\n",
      "Training T\n",
      "  (took 69.9556 seconds)\n",
      "Training V\n",
      "  (took 71.6595 seconds)\n",
      "Training A\n",
      "  (took 76.1969 seconds)\n",
      "Training .\n",
      "  (took 70.936 seconds)\n"
     ]
    }
   ],
   "source": [
    "# A test/train split - train with [0:split], test with [split:len(gmb)]\n",
    "split = int(len(gmb) * 0.3) # Have a lot of data, and don't want you waiting around too long to train!\n",
    "print('Using {} sentences for training'.format(split))\n",
    "\n",
    "\n",
    "def train_tag_model(tag):    \n",
    "#train each tag and store probability to dictionary\n",
    "\n",
    "    tagNum = rpos_to_num[tag]\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #create training y for each tag. make 1 if tag corresponds to training tag else make 0\n",
    "    y = np.copy(trainingY)\n",
    "    \n",
    "    for i in range(y.shape[0]):\n",
    "        if y[i] == tagNum:\n",
    "            y[i] = 1\n",
    "        else:\n",
    "            y[i] = 0\n",
    "    #y[y!=tagNum]=int(0)\n",
    "    #y[y==tagNum]=int(1)\n",
    "    \n",
    "    \n",
    "    alpha, cost, finalPrediction = nesterov(ex, y)\n",
    "\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "    print('  (took {:g} seconds)'.format(end-start))\n",
    "    \n",
    "    return alpha\n",
    "    \n",
    "\n",
    "    \n",
    "   \n",
    " \n",
    "# Code to train a model for each reduced POS tag...\n",
    "rpos_model = {}\n",
    "for tag in rpos_desc:\n",
    "    print('Training {}'.format(tag))\n",
    "    rpos_model[tag] = train_tag_model(tag)\n",
    "\n",
    "alphaDict = rpos_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we will train the models above, then estimate POS tags...\n",
    "def token_pos(sentence):\n",
    "    \"\"\"Given a sentence, as a list of tokens, this should return part of\n",
    "    speech tags, as a list of strings (the codes in the rpos_desc dictionary).\n",
    "    Basically calls the models for each tag and selects the tag with the\n",
    "    highest probability.\"\"\"\n",
    "    \n",
    "    #create ex word vector for each word in sentence. length is num words by 601\n",
    "\n",
    "    xList = []\n",
    "    wordsList = []\n",
    "    for word in sentence:\n",
    "        x = glove.decode(word)\n",
    "        xList.append(x)\n",
    "    x = np.array(xList)\n",
    " \n",
    "    nf = numpy.sin(numpy.einsum('ef,gf->eg', x , w))\n",
    "    \n",
    "    ex = numpy.append(x,nf, axis=1)\n",
    "    \n",
    "                                \n",
    "    ones = np.ones(x.shape[0])[:,None]\n",
    "    ex = numpy.append(ones,ex, axis=1)\n",
    "    \n",
    "    predictionArray = np.zeros([len(sentence),16])\n",
    "    \n",
    "    #calculate prediction for each tag using alphas calculated \n",
    "    #returns matrix of number of words by 16\n",
    "    for tag, alpha in alphaDict.items():\n",
    "        pred = prediction(ex,alpha)\n",
    "        tagNum = rpos_to_num[tag]\n",
    "        predictionArray[:,tagNum] = pred\n",
    "    \n",
    "    #take maximum value of tag as the final prediction and append to a list for each word\n",
    "\n",
    "    wordPos = []\n",
    "    for i in range(predictionArray.shape[0]):\n",
    "\n",
    "        bestProb = np.argmax(predictionArray[i])\n",
    "        \n",
    "        rpos = num_to_rpos[bestProb]\n",
    "\n",
    "        wordPos.append(rpos)\n",
    "\n",
    "\n",
    "\n",
    "    return wordPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%\n",
      "Percentage correct = 93.0%\n",
      "  (took 87.3445 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Code to test the performance of POS tagger...\n",
    "correct = 0\n",
    "tested = 0\n",
    "pershown = 0\n",
    "\n",
    "# can finish earlier if taking too long\n",
    "stop_percent = 100\n",
    "\n",
    "start = time.time()\n",
    "for i in range(split, len(gmb)):\n",
    "    percent = int(100 * (i - split) / (len(gmb) - split))\n",
    "    if percent>pershown:\n",
    "        pershown = percent\n",
    "        print('\\r{: 3d}%'.format(percent), end='')\n",
    "    \n",
    "    if percent>=stop_percent:\n",
    "        break\n",
    "    \n",
    "    guess = token_pos(gmb[i])\n",
    "\n",
    "    truth = gmb.pos(i)\n",
    "    \n",
    "    for g,t in zip(guess, truth):\n",
    "        if g==pos_to_rpos[t]:\n",
    "            correct += 1\n",
    "        tested += 1\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print('Percentage correct = {:.1f}%'.format(100 * correct / tested))\n",
    "print('  (took {:g} seconds)'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Part of speech tagging - sentence level\n",
    "\n",
    "While the previous step works very well we need POS tags to be super accurate, as everything else depends on them. We will now introduce context. This is done by calculating transition probabilities between tags and solving a Markov random chain using the forward-backwards algorithm to find the maximum a posteriori (MAP) POS tag assignment for the entire sentence. The adjacency matrix contains $\\log P(\\textrm{second pos tag} | \\textrm{first pos tag})$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateAdjacencies():\n",
    "    \n",
    "    #create matrix which calculates the adjacent probabilityies.\n",
    "    \n",
    "    #initialise matrix\n",
    "    adMat = np.ones([16,16])\n",
    "    \n",
    "    #loop through all words and count the number of times one tag follows another\n",
    "    wordNum = 0\n",
    "    for i in range(len(posWordList)-1):\n",
    "            \n",
    "        word = posWordList[i]\n",
    "        nextWord = posWordList[i+1]\n",
    "\n",
    "        adMat[word,nextWord]+=1\n",
    "        \n",
    "        \n",
    "    #normalise by taking percentage for word\n",
    "    posSum = np.sum(adMat,axis=1).reshape(-1,1)\n",
    "    \n",
    "\n",
    "    adMat = adMat/posSum\n",
    " \n",
    "    #return log of the matrix\n",
    "    adMat = np.log(adMat)\n",
    "    \n",
    "    return adMat\n",
    "\n",
    "admat = CreateAdjacencies()   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emissionMatrix(sentence):\n",
    "    \"\"\"Given a sentence, as a list of tokens, this should return part of\n",
    "    speech tags, as a list of strings (the codes in the rpos_desc dictionary).\n",
    "    Basically calls the models for each tag and selects the tag with the\n",
    "    highest probability.\"\"\"\n",
    "    \n",
    "    \n",
    "    #create emission matrix using part one alphas to predict tag for each word without context\n",
    "\n",
    "    xList = []\n",
    "    wordsList = []\n",
    "    for word in sentence:\n",
    "\n",
    "        x = glove.decode(word)\n",
    "        xList.append(x)\n",
    "    x = np.array(xList)\n",
    "\n",
    "    nf = numpy.sin(numpy.einsum('ef,gf->eg', x , w))\n",
    "    \n",
    "    ex = numpy.append(x,nf, axis=1)\n",
    "    \n",
    "                                \n",
    "    ones = np.ones(x.shape[0])[:,None]\n",
    "    ex = numpy.append(ones,ex, axis=1)\n",
    "    \n",
    "    #calculate probability of word for each tag\n",
    "    predictionArray = np.zeros([len(sentence),16])\n",
    "    \n",
    "    for tag, alpha in alphaDict.items():\n",
    "        pred = prediction(ex,alpha)\n",
    "        tagNum = rpos_to_num[tag]\n",
    "        predictionArray[:,tagNum] = pred\n",
    "    \n",
    "    sumPred = np.sum(predictionArray,axis=1).reshape(-1,1)\n",
    "    \n",
    "    predictionArray = predictionArray/sumPred\n",
    "   \n",
    "    \n",
    "    return predictionArray\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_pos(sentence):\n",
    "    \"\"\"Given a sentence, as a list of tokens, this should return part of\n",
    "    speech tags, as a list of strings (the codes in the rpos_desc dictionary).\n",
    "    A more advanced version of token_pos that uses neighbours as well.\"\"\"\n",
    "    \n",
    "   #use forward backward algorithm to calculate the conditional probability of tag given the rest of the words in the sentence\n",
    "    \n",
    "    fwd = []\n",
    "    f_prev = {}\n",
    "    #return emission matrix\n",
    "    emMatrix = emissionMatrix(sentence)\n",
    "    emMatrix = np.log(emMatrix)\n",
    "    \n",
    "    forwards = np.zeros([len(sentence),len(rpos_to_num)])\n",
    "    #loops through time step and states\n",
    "    \n",
    "    #tags are states and words are observable\n",
    "    for t, word in enumerate(sentence):\n",
    "        f_curr = {}\n",
    "    \n",
    "        if t == 0:\n",
    "            # base case for the forward part\n",
    "\n",
    "            forwards[t] = emMatrix[t]\n",
    "\n",
    "        else:\n",
    "            \n",
    "            for key, s  in rpos_to_num.items():\n",
    "                prev_f_sum = np.zeros(len(rpos_to_num))\n",
    "                for key, sPrev in rpos_to_num.items():\n",
    "                    prev_f_sum[sPrev] = forwards[t-1,sPrev]+admat[sPrev,s]+emMatrix[t,s]\n",
    "\n",
    "                forwards[t,s] = numpy.max(prev_f_sum)\n",
    "\n",
    "\n",
    "\n",
    "    #backwards\n",
    "    backwards = np.zeros([len(sentence),len(rpos_to_num)])\n",
    "    \n",
    "    for t in reversed(range(len(sentence))):\n",
    "    #for st in rpos_desc:\n",
    "\n",
    "        if t == len(sentence)-1:\n",
    "            # base case uses the forwards \n",
    "\n",
    "            backwards[t] = forwards[t]\n",
    "\n",
    "        else:\n",
    "            \n",
    "            for key, s  in rpos_to_num.items():\n",
    "                prev_f_sum = np.zeros(len(rpos_to_num))\n",
    "                for key, sPrev in rpos_to_num.items():\n",
    "                    prev_f_sum[sPrev] = backwards[t+1,sPrev]+admat[s,sPrev]+emMatrix[t,s]\n",
    "\n",
    "                backwards[t,s] = numpy.max(prev_f_sum)\n",
    "    \n",
    "    #return highest index of backward as prediction for the tag\n",
    "    finalPredIndex = numpy.argmax(backwards, axis=1)\n",
    "    \n",
    "    finalPred = []\n",
    "    \n",
    "    for i in finalPredIndex:\n",
    "        finalPred.append(num_to_rpos[i])\n",
    "        \n",
    "        \n",
    "    return finalPred\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%\n",
      "Percentage correct = 93.6%\n",
      "  (took 496.841 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Code to test the performance of your improved POS tagger...\n",
    "correct = 0\n",
    "tested = 0\n",
    "pershown = 0\n",
    "stop_percent = 100\n",
    "\n",
    "start = time.time()\n",
    "for i in range(split, len(gmb)):\n",
    "    percent = int(100 * (i - split) / (len(gmb) - split))\n",
    "    if percent>pershown:\n",
    "        pershown = percent\n",
    "        print('\\r{: 3d}%'.format(percent), end='')\n",
    "    \n",
    "    if percent>=stop_percent:\n",
    "        break\n",
    "    \n",
    "    guess = sentence_pos(gmb[i])\n",
    "    truth = gmb.pos(i)\n",
    "    \n",
    "    for g,t in zip(guess, truth):\n",
    "        if g==pos_to_rpos[t]:\n",
    "            correct += 1\n",
    "        tested += 1\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print('Percentage correct = {:.1f}%'.format(100 * correct / tested))\n",
    "print('  (took {:g} seconds)'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition\n",
    "\n",
    "The next step is to identify names, that is the entities that \"facts\" may apply to. While training a further classifier does work (same as above, inc. dynamic programming) there would be little point in repeating the exercise. Instead, a simple rule based approach using *regular expressions* is going to be used.\n",
    "\n",
    "\n",
    "Given part of speech tagging a name can be defined as:\n",
    "* An optional *determiner*, e.g. *the* (1 or none)\n",
    "* An arbitrary number of *adjectives* (could be none)\n",
    "* A single *noun*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_ner(sentence, pos):\n",
    "    \"\"\"Given a sentence as a list of tokens and it's part of speech tags\n",
    "    this returns a list of the same length with True wherever it thinks\n",
    "    there is a name.\"\"\"\n",
    "    \n",
    "    #return array of true and false for each word if it follows the rules above\n",
    "    ret = [False] * len(sentence)\n",
    "    \n",
    "    #regular expression for names\n",
    "    regex = 'D?J*N'\n",
    "    \n",
    "    #create string of tags\n",
    "    joinPos = ''.join(pos)\n",
    "    #check for regex matches in the string\n",
    "    for match in re.finditer(regex, joinPos):\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        #for all matches make value true \n",
    "        for a in range(start,end):\n",
    "            ret[a] = True\n",
    "    \n",
    "\n",
    "    \n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99%\n",
      "Percentage correct = 63.4%\n",
      "  (took 0.532609 seconds)\n"
     ]
    }
   ],
   "source": [
    "# Code to test the performance of the NER tagger...\n",
    "correct = 0\n",
    "tested = 0\n",
    "pershown = 0\n",
    "stop_percent = 100 # If you want faster feedback you can reduce this\n",
    "\n",
    "start = time.time()\n",
    "for i in range(split, len(gmb)):\n",
    "    percent = int(100 * (i - split) / (len(gmb) - split))\n",
    "    if percent>pershown:\n",
    "        pershown = percent\n",
    "        print('\\r{: 3d}%'.format(percent), end='')\n",
    "    \n",
    "    if percent>=stop_percent:\n",
    "        break\n",
    "    \n",
    "    guess = sentence_ner(gmb[i], [pos_to_rpos[p] for p in gmb.pos(i)])\n",
    "    truth = [ner!='O' for ner in gmb.ner(i)]\n",
    "    \n",
    "    for g,t in zip(guess, truth):\n",
    "        if g==t:\n",
    "            correct += 1\n",
    "        tested += 1\n",
    "end = time.time()\n",
    "\n",
    "print()\n",
    "print('Percentage correct = {:.1f}%'.format(100 * correct / tested))\n",
    "print('  (took {:g} seconds)'.format(end - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation extraction\n",
    "\n",
    "This is where the paper \"*Identifying Relations for Open Information Extraction*\" comes in, specifically one of its novel contributions. It extracts relations using this procedure:\n",
    "1. Find relation text by matching a human-designed pattern to the POS tags\n",
    "2. Identify the named entities to the left and right of the relation text.\n",
    "3. Generate the relation tuple (left named entity, relation text, right named entity)\n",
    "\n",
    "(all previous approaches found names then relations - turns out it works much better the other way around)\n",
    "\n",
    "Relation text is identified as:\n",
    "`(Ve (Wo* Pa)?)+`\n",
    "where\n",
    "* `Ve = Verb Particle? Adverb?`\n",
    "* `Wo = Noun | Adjective | Adverb | Pronoun | Determiner`\n",
    "* `Pa = Preposition or subordinating conjunction | Particle`\n",
    "* `| =` or, so either of the options\n",
    "* `? =` optional\n",
    "* `+ =` at least one, but can be many\n",
    "* `* =` an arbitrary number of repetitions, including the option for none.\n",
    "\n",
    "We will need to convert the above rules into a regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(sentence):\n",
    "    \"\"\"Given a sentence, as a list of tokens, this returns a list of all relations\n",
    "    extracted from the sentence. Each relation is a tuple with three entries:\n",
    "    (named entity one, relation, named entity two)\"\"\"\n",
    "    pos = sentence_pos(sentence)\n",
    "\n",
    "    ner = sentence_ner(sentence, pos)\n",
    "    \n",
    "    #create regex expression for relation\n",
    "    ret = []\n",
    "    Ve = '(VZ?R?)'\n",
    "    Wo = '(N|J|R|M|D)'\n",
    "    Pa = '(I|Z)'\n",
    "    regex = '({}({}*{})?)+'.format(Ve,Wo,Pa)\n",
    "    \n",
    "    #create string of tags\n",
    "    joinPos = ''.join(pos)\n",
    "    \n",
    "    for match in re.finditer(regex, joinPos):\n",
    "        \n",
    "\n",
    "        start = match.start()\n",
    "        \n",
    "        end = match.end()\n",
    "        \n",
    "        relation = sentence[start:end]\n",
    "        \n",
    "        #check value before and after the relation string to check if the named entity before or after\n",
    "        leftEnt = []\n",
    "        leftInd = []\n",
    "        \n",
    "        #check if tags before the relation are named entities\n",
    "        for i in reversed(range(0,start)):\n",
    "            if ner[i] ==True:\n",
    "                leftInd.append(i)\n",
    "                \n",
    "            else:\n",
    "                break\n",
    "        leftInd = np.sort(leftInd)\n",
    "        \n",
    "        \n",
    "        for j in range(len(leftInd)):\n",
    "            \n",
    "            leftEnt.append(sentence[j])\n",
    "            \n",
    "        #check if tags after the relation are named entities\n",
    "        rightEnt = []\n",
    "     \n",
    "        for i in range(end,len(sentence)):\n",
    "            \n",
    "            \n",
    "            if ner[i] ==True:\n",
    "                rightEnt.append(sentence[i])\n",
    "            else:\n",
    "                break\n",
    "   \n",
    "        if len(leftInd) == 0 or len(rightEnt)==0:\n",
    "            break\n",
    "        else:\n",
    "            #create tuple of named left, named right and relation if left and right exist\n",
    "            retTup = (' '.join(leftEnt),' '.join(relation),' '.join(rightEnt))\n",
    "           \n",
    "            ret.append(retTup)\n",
    "        \n",
    "    \n",
    "    return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London is full of pigeons.\n",
      "  London -- is full of -- pigeons\n",
      "\n",
      "In 1781 William Herschel discovered Uranus\n",
      "  In 1781 William -- discovered -- Uranus\n",
      "\n",
      "Trolls really don't like the sun.\n",
      "\n",
      "Giant owls would enjoy eatting people.\n",
      "\n",
      "Dragons collect gold, but they don't make microprocessors.\n",
      "  Dragons -- collect -- gold\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small test of the above...\n",
    "tests = ['London is full of pigeons.',\n",
    "         'In 1781 William Herschel discovered Uranus', # 1\n",
    "         \"Trolls really don't like the sun.\",\n",
    "         'Giant owls would enjoy eatting people.',\n",
    "         \"Dragons collect gold, but they don't make microprocessors.\"] # 2\n",
    "\n",
    "# 1. Seems to miss William - misclassified it, at least with the model answer.\n",
    "# 2. Should extract two facts, first sensible, second absurd.\n",
    "\n",
    "for sentence in tests:\n",
    "    print(sentence)\n",
    "    tokens = ogonek.Tokenise(sentence)\n",
    "    \n",
    "    rels = extract(tokens[0])\n",
    "    for rel in rels:\n",
    "        print('  ' + ' -- '.join(rel))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20000 relations under the seas\n",
    "\n",
    "While the above may have tested each step of the system, the below code runs it on the book \"*20,000 leagues under the seas*\" by Jules Verne (widely considered to be the first science fiction book, and full of fairly dubious claims about what goes on underwater)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(under_the_seas)):\n",
    "    sentence = under_the_seas[index]\n",
    "    rels = extract(sentence)\n",
    "    \n",
    "    if len(rels)>0:\n",
    "        print(' '.join(sentence))\n",
    "        for rel in rels:\n",
    "            print('  ' + ' -- '.join(rel))\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
